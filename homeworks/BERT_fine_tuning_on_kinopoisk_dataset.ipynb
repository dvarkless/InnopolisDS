{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dvarkless/InnopolisDS/blob/main/homeworks/BERT_fine_tuning_on_kinopoisk_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "8a4PVnfQze",
        "id": "fDd9Aj6bJEmA"
      },
      "source": [
        "Sentiment analysis on Kinopoisk movie reviews dataset using pretrained BERT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "jdJD0kgdwf",
        "id": "LC6u1wBcJEmD"
      },
      "source": [
        "Импорты: pytorch, bert, вспомогательные библиотеки, шкала прогресса, отключение предупреждений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "6gUAGkjX5g",
        "id": "6st-EzRWJEmE"
      },
      "source": [
        "import shutup; shutup.please()\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from alive_progress import alive_bar\n",
        "\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    n_gpu = torch.cuda.device_count()\n",
        "    torch.cuda.get_device_name(0)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "XIK5TgHpDq",
        "id": "VqjlDnRyJEmF"
      },
      "source": [
        "DATA_DIR=\"../LSTM Sentiment/\"\n",
        "PROP = 0.2"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "ttNy7LFHr3",
        "id": "0w6ebjPyJEmG"
      },
      "source": [
        "Класс кастомного датасета позволит токенизировать текст по мере необходимости"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "oOWb6j5g3N",
        "id": "iyoDRjdKJEmG"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, texts, targets, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.class_weights = {name: 1-(count/len(self)) for name, count in self.targets.value_counts().items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, int):\n",
        "            idx = idx if idx < len(self) else len(self)-1\n",
        "        text = str(self.texts[idx])\n",
        "        target = self.targets[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "          'text': text,\n",
        "          'input_ids': encoding['input_ids'].flatten(),\n",
        "          'attention_mask': encoding['attention_mask'].flatten(),\n",
        "          'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def get_weights(self):\n",
        "        return [self.class_weights[sample] for sample in self.targets]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "wGqITVoinv",
        "id": "ItUuGdl7JEmH"
      },
      "source": [
        "dataset_path = DATA_DIR / Path('dataset')\n",
        "\n",
        "class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "class_names_converter = {\n",
        "    'neg': 'Negative',\n",
        "    'pos': 'Positive',\n",
        "    'neu': 'Neutral',\n",
        "}\n",
        "\n",
        "def standardize_text(df, content_field):\n",
        "    df[content_field] = df[content_field].str.replace(r\"http\\S+\", \"\")\n",
        "    df[content_field] = df[content_field].str.replace(r\"@\\S+\", \"\")\n",
        "    df[content_field] = df[content_field].str.replace(\n",
        "        r\"[^А-Яа-яA-Za-z0-9Ёё(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
        "    df[content_field] = df[content_field].str.replace(r\"[Ёё]\", \"е\")\n",
        "    df[content_field] = df[content_field].str.replace(r\"[\\t\\n]\", \"\")\n",
        "    df[content_field] = df[content_field].str.replace(r\"[^А-Яа-яa-zA-Z]\", \" \")\n",
        "    df[content_field] = df[content_field].str.lower()\n",
        "    return df\n",
        "\n",
        "def name_to_id(name):\n",
        "    return class_names.index(name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "yyU0dc4G2y",
        "id": "NV2BRC4hJEmI"
      },
      "source": [
        "Загрузим датасет отзывов с кинопоиска в датафрейм Пандас  \n",
        "Для этого надо пройтись по каждой директории, открыть текстовый файл и загрузить его содержимое в датафрейм  \n",
        "Далее идет стандартизация текста и разбиение его на тестовую и проверочную выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "miTk5oP3Qe",
        "id": "94Rb9iofJEmI",
        "outputId": "5c7ce975-66a4-4ab9-a9aa-66ba72ddd7b0"
      },
      "source": [
        "for perc in [0.1, 0.2, 0.5]:\n",
        "\n",
        "    df = pd.DataFrame(columns=['review', 'sentiment'])\n",
        "    for class_path in dataset_path.iterdir():\n",
        "\n",
        "        if class_path.is_dir():\n",
        "            dirs = np.array(list(class_path.iterdir()))\n",
        "            np.random.shuffle(dirs)\n",
        "            rews_fhs = np.random.choice(dirs, round(len(dirs)*perc))\n",
        "            print(f'len = {rews_fhs.shape}')\n",
        "            print(class_names_converter[class_path.name])\n",
        "            for rew_fh in rews_fhs:\n",
        "                with open(Path(rew_fh), encoding='utf-8') as f:\n",
        "                    review = f.read()\n",
        "                    current_df = pd.DataFrame(\n",
        "                        {'review': [review], 'sentiment': class_names_converter[class_path.name]})\n",
        "                    df = pd.concat([df, current_df], ignore_index=True)\n",
        "\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    df = standardize_text(df, \"review\")\n",
        "    df['sentiment'] = df['sentiment'].map(name_to_id)\n",
        "\n",
        "    train_dataset, eval_dataset = train_test_split(df, test_size=PROP)\n",
        "\n",
        "    train_dataset.to_csv(f'Kinopoisk_train_{perc:.0%}.csv')\n",
        "    eval_dataset.to_csv(f'Kinopoisk_eval_{perc:.0%}.csv')"
      ],
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len = (2470,)\n",
            "Neutral\n",
            "len = (8714,)\n",
            "Positive\n",
            "len = (1983,)\n",
            "Negative\n",
            "len = (4941,)\n",
            "Neutral\n",
            "len = (17428,)\n",
            "Positive\n",
            "len = (3965,)\n",
            "Negative\n",
            "len = (12352,)\n",
            "Neutral\n",
            "len = (43569,)\n",
            "Positive\n",
            "len = (9914,)\n",
            "Negative\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "dGsaWqQbqW",
        "id": "za1hxSb2JEmK"
      },
      "source": [
        "Так как датасет был конвертирован в csv заранее, то пропустим выполнение строк выше и просто загрузим выборки из файла, который находится в локальной машине"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "hu9qwmWZTB",
        "id": "XSSS4cJrJEmK"
      },
      "source": [
        "train_datasets = []\n",
        "eval_datasets = []\n",
        "\n",
        "for perc in [0.1, 0.2, 0.5, 1]:\n",
        "    train_dataset = pd.read_csv(f'Kinopoisk_train_{perc:.0%}.csv')\n",
        "    eval_dataset = pd.read_csv(f'Kinopoisk_eval_{perc:.0%}.csv')\n",
        "    train_datasets.append(train_dataset)\n",
        "    eval_datasets.append(eval_dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "A30BrDHHyO",
        "id": "sZgDpUHSJEmL"
      },
      "source": [
        "Инициируем токенайзер, модель и оборачиваем датасет в кастомный класс\n",
        "Используем модель rubert-tiny2, тк она лучше всего подходит для задач NLP на русском языке"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "FgZV68751q",
        "id": "zGqugXdDJEmL"
      },
      "source": [
        "rubert_path = 'cointegrated/rubert-tiny2'\n",
        "tokenizer = BertTokenizer.from_pretrained(rubert_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "gNRs7TYZsc",
        "id": "PrfxL6zTJEmM"
      },
      "source": [
        "def convert_ds(dataset):\n",
        "    return CustomDataset(dataset['review'], dataset['sentiment'], tokenizer, max_len=512)\n",
        "\n",
        "train_datasets = [convert_ds(dataset) for dataset in train_datasets]\n",
        "eval_datasets = [convert_ds(dataset) for dataset in eval_datasets]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "SJLlzNhzAS",
        "id": "9hEi0EJkJEmM"
      },
      "source": [
        "n_classes = 3\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(rubert_path)\n",
        "\n",
        "out_features = model.bert.encoder.layer[1].output.dense.out_features\n",
        "model.classifier = torch.nn.Linear(out_features, n_classes)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "tzpA0ZiE59",
        "id": "1_kAOiY3JEmM"
      },
      "source": [
        "После определения модели можно определить оптимизатор, функцию потерь\n",
        "и класс, который будет обучать и проверять модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "sd1WcvbKLu",
        "id": "IJ_UKS56JEmN"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "94WXLihbFt",
        "id": "S_LbUZ3dJEmN"
      },
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss().to(device)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "6Nndlb083q",
        "id": "PW3nKn5WJEmN"
      },
      "source": [
        "class BertClassifier:\n",
        "    def __init__(self, model, tokenizer, optimizer, loss, n_classes=3, max_len=512, model_save_path='rubert_on_kinopoisk.pt', log=True):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.optimizer = optimizer\n",
        "        self.loss = loss\n",
        "        self.loss.to(self.device)\n",
        "        self.model_save_path=model_save_path\n",
        "        self.max_len = max_len\n",
        "        self.log = log\n",
        "        self.model.to(self.device)\n",
        "    \n",
        "    def fit(self, train, eval, epochs=2, save_model=True):\n",
        "        self.model = self.model.train()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "        best_accuracy = 0\n",
        "\n",
        "        # sampler = torch.utils.data.WeightedRandomSampler(train.get_weights(), num_samples=len(train), replacement=True)\n",
        "        train_dl = DataLoader(train, batch_size=8, sampler=torch.utils.data.RandomSampler(train))\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "                self.optimizer,\n",
        "                num_warmup_steps=0,\n",
        "                num_training_steps=len(train_dl) * epochs\n",
        "            )\n",
        "\n",
        "        with alive_bar(epochs*len(train)//8 + 1, title=f'Обучение модели ruBERT', force_tty=True, bar='filling') as bar:\n",
        "            for epoch in range(epochs):\n",
        "\n",
        "                correct_predictions = 0\n",
        "                for data in train_dl:\n",
        "\n",
        "                    input_ids = data[\"input_ids\"].to(self.device)\n",
        "                    attention_mask = data[\"attention_mask\"].to(self.device)\n",
        "                    targets = data[\"targets\"].to(self.device)\n",
        "\n",
        "                    outputs = self.model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask\n",
        "                        )\n",
        "\n",
        "                    preds = torch.argmax(outputs.logits, dim=1)\n",
        "                    loss = self.loss(outputs.logits, targets)\n",
        "\n",
        "                    correct_predictions += torch.sum(preds == targets)\n",
        "\n",
        "                    losses.append(loss.item())\n",
        "                    \n",
        "                    loss.backward()\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "                    self.optimizer.step()\n",
        "                    scheduler.step()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    bar()\n",
        "\n",
        "                train_acc = correct_predictions.double() / len(train)\n",
        "                train_loss = np.mean(losses)\n",
        "\n",
        "                eval_acc, eval_loss = self.eval(eval)\n",
        "                \n",
        "                if self.log:\n",
        "                    print(f'===========Epoch {epoch+1}/{epochs}===========')\n",
        "                    print(f'Train loss: {train_loss:.4f} accuracy: {train_acc:.4f}')\n",
        "\n",
        "                    print(f'Val loss {eval_loss:.4f} accuracy {eval_acc:.4f}')\n",
        "\n",
        "                if save_model:\n",
        "                    if eval_acc > best_accuracy:\n",
        "                        torch.save(self.model, self.model_save_path)\n",
        "                        best_accuracy = eval_acc\n",
        "\n",
        "        return self \n",
        "\n",
        "    def eval(self, eval):\n",
        "        self.model = self.model.eval()\n",
        "        losses = []\n",
        "        correct_predictions = 0\n",
        "\n",
        "        eval_dl = DataLoader(eval, batch_size=32, sampler=torch.utils.data.SequentialSampler(eval))\n",
        "        with torch.no_grad():\n",
        "            for data in eval_dl:\n",
        "                input_ids = data[\"input_ids\"].to(self.device)\n",
        "                attention_mask = data[\"attention_mask\"].to(self.device)\n",
        "                targets = data[\"targets\"].to(self.device)\n",
        "\n",
        "                outputs = self.model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask\n",
        "                    )\n",
        "\n",
        "                preds = torch.argmax(outputs.logits, dim=1)\n",
        "                loss = self.loss(outputs.logits, targets)\n",
        "                correct_predictions += torch.sum(preds == targets)\n",
        "                losses.append(loss.item())\n",
        "        \n",
        "        val_acc = correct_predictions.double() / len(eval)\n",
        "        val_loss = np.mean(losses)\n",
        "        return val_acc, val_loss\n",
        "    \n",
        "    def predict(self, text):\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        \n",
        "        out = {\n",
        "              'text': text,\n",
        "              'input_ids': encoding['input_ids'].flatten(),\n",
        "              'attention_mask': encoding['attention_mask'].flatten()\n",
        "          }\n",
        "        \n",
        "        input_ids = out[\"input_ids\"].to(self.device)\n",
        "        attention_mask = out[\"attention_mask\"].to(self.device)\n",
        "        \n",
        "        outputs = self.model(\n",
        "            input_ids=input_ids.unsqueeze(0),\n",
        "            attention_mask=attention_mask.unsqueeze(0)\n",
        "        )\n",
        "        \n",
        "        prediction = torch.argmax(outputs.logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "        return prediction"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "rYT7ZKqJ9g",
        "id": "D_2mZ40iJEmO"
      },
      "source": [
        "обучение модели, сохранение обученной модели в файле"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "cpSBILUvCR",
        "id": "MGdcCGSYJEmO",
        "outputId": "f419fdf1-b4d2-4407-c3af-b1fe368ea481"
      },
      "source": [
        "num_epochs = [1, 2, 2, 4]\n",
        "model_names = ['ds_10%', 'ds_20%', 'ds_50%', 'ds_100%']\n",
        "n_classes = 3\n",
        "\n",
        "\n",
        "for num_epochs, model_name, train_ds, eval_ds in zip(num_epochs, model_names, train_datasets, eval_datasets):\n",
        "    model = BertForSequenceClassification.from_pretrained(rubert_path)\n",
        "\n",
        "    out_features = model.bert.encoder.layer[1].output.dense.out_features\n",
        "    model.classifier = torch.nn.Linear(out_features, n_classes)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    BertClassifier(model, tokenizer, optimizer, loss_fn, model_save_path=f'rubert_on_kinopoisk_{model_name}.pt').fit(train_ds, eval_ds, epochs=num_epochs)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "on 1317: ===========Epoch 1/1===========\non 1317: Train loss: 0.7063 accuracy: 0.7056\non 1317: Val loss 0.6325 accuracy 0.7403\non 2634: ===========Epoch 1/2===========\non 2634: Train loss: 0.6700 accuracy: 0.7227\non 2634: Val loss 0.5968 accuracy 0.7537\non 5268: ===========Epoch 2/2===========\non 5268: Train loss: 0.5978 accuracy: 0.7891\non 5268: Val loss 0.5917 accuracy 0.7600\non 6584: ===========Epoch 1/2===========\non 6584: Train loss: 0.6257 accuracy: 0.7402\non 6584: Val loss 0.5629 accuracy 0.7727\non 13168: ===========Epoch 2/2===========\non 13168: Train loss: 0.5580 accuracy: 0.8061\non 13168: Val loss 0.5497 accuracy 0.7873\non 13167: ===========Epoch 1/4===========\non 13167: Train loss: 0.5903 accuracy: 0.7576\non 13167: Val loss 0.5103 accuracy 0.7919\non 26334: ===========Epoch 2/4===========\non 26334: Train loss: 0.5075 accuracy: 0.8405\non 26334: Val loss 0.4836 accuracy 0.8284\non 39501: ===========Epoch 3/4===========\non 39501: Train loss: 0.4408 accuracy: 0.9129\non 39501: Val loss 0.6205 accuracy 0.8571\non 52668: ===========Epoch 4/4===========\non 52668: Train loss: 0.3808 accuracy: 0.9541\non 52668: Val loss 0.7209 accuracy 0.8649\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "dAU32vMHuC",
        "id": "iI_EZq-9JEmO",
        "outputId": "9d4efda2-63df-4d30-c8eb-033fd0d434a3"
      },
      "source": [
        "model_name = model_names[0]\n",
        "model = torch.load(f'rubert_on_kinopoisk_{model_name}.pt')\n",
        "acc, loss = BertClassifier(model, tokenizer, optimizer, loss_fn).eval(eval_datasets[-1])\n",
        "print(f'({model_name}) model\\'s accuracy is === {acc:.2%} === with average loss = {loss:.5f}' )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(ds_10%) model's accuracy is === 73.22% === with average loss = 0.64830\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "I6G23wR9PI",
        "id": "r0LGeqUpJEmP",
        "outputId": "914c6d08-341d-4421-d59a-df3ce2dc0fbf"
      },
      "source": [
        "model_name = model_names[1]\n",
        "model = torch.load(f'rubert_on_kinopoisk_{model_name}.pt')\n",
        "acc, loss = BertClassifier(model, tokenizer, optimizer, loss_fn).eval(eval_datasets[-1])\n",
        "print(f'({model_name}) model\\'s accuracy is === {acc:.2%} === with average loss = {loss:.5f}' )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(ds_20%) model's accuracy is === 75.36% === with average loss = 0.59896\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "mWry8AZ2aK",
        "id": "DYNzcPFiJEmP",
        "outputId": "e981cef3-52ee-4f3a-d496-e402fe3c211f"
      },
      "source": [
        "model_name = model_names[2]\n",
        "model = torch.load(f'rubert_on_kinopoisk_{model_name}.pt')\n",
        "acc, loss = BertClassifier(model, tokenizer, optimizer, loss_fn).eval(eval_datasets[-1])\n",
        "print(f'({model_name}) model\\'s accuracy is === {acc:.2%} === with average loss = {loss:.5f}' )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(ds_50%) model's accuracy is === 78.35% === with average loss = 0.54470\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "Ra3AWK67QO",
        "id": "7_eGhYFiJEmP",
        "outputId": "a8909ee5-c099-47ad-b3ab-e87736311170"
      },
      "source": [
        "model_name = model_names[3]\n",
        "model = torch.load(f'rubert_on_kinopoisk_{model_name}.pt')\n",
        "acc, loss = BertClassifier(model, tokenizer, optimizer, loss_fn).eval(eval_datasets[-1])\n",
        "print(f'({model_name}) model\\'s accuracy is === {acc:.2%} === with average loss = {loss:.5f}' )"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "(ds_100%) model's accuracy is === 86.49% === with average loss = 0.72091\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "vNovJXkUnM",
        "id": "aoIAX8TvJEmP"
      },
      "source": [
        "Проверка моделей.\n",
        "Данные взяты из Кинопоиска, отзывы на недавно вышедшие фильмы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "PmkGsh5ccF",
        "id": "oOmSgPoqJEmQ",
        "outputId": "afc848a8-188b-4f93-d09e-ead1b185d3b3"
      },
      "source": [
        "for suffix in model_names:\n",
        "# Отрицательный отзыв на фильм Морбиус (2022) \n",
        "    model = torch.load(f'rubert_on_kinopoisk_{suffix}.pt')\n",
        "    model_runner = BertClassifier(model, tokenizer, optimizer, loss_fn)\n",
        "    print(f'======================= ruBert with {suffix} dataset volume =========================')\n",
        "    header = \"Morbius 2022\"\n",
        "    review = \"Ну, я даже не знаю, какой супергеройский фильм последних лет может хотя бы на толику быть на столько ужасным. Абсолютно не спасает этот плевок в сторону фан-сообщества наличие нестареющего (видимо действительно вампирского происхождения) Джареда Лето. Хотя, чего уж скрывать, он всё равно красавчик, и эта роль ему на все сто процентов подходит, НО только при наличии адекватного сценария и не отвлекающегося на перекуры и другие интересные дела режиссера\"\n",
        "    correct = \"Negative\"\n",
        "    model_out = model_runner.predict(review)\n",
        "    print(f'{\"V\" if class_names[model_out]==correct else \"X\"} Результат модели: This review about \"{header}\" is {class_names[model_out]} (in fact it is {correct})')\n",
        "\n",
        "# Положительный Avengers: Endgame, 2019\n",
        "    header = \"Avengers: Endgame, 2019\"\n",
        "    review = \"Есть фильмы, которые хороши не потому, что в них все идеально. Картина может иметь сотню недостатков, куча дыр и несостыковок, завышенные ожидания со стороны зрителей, но все равно цепляет и оставляет по себе очень приятное послевкусие. Именно к таким фильмам лично я для себя причисляю Мстителей. Финал'. Просматривая его третий раз в кино, я поняла, что еще раз и смогу разложить и сюжет и мотивацию героев по атомам, тем не менее, ни ненависти ни какого-то огорчения я не испытываю. Если бы меня спросили, что бы я поменяла, то определенно получилось бы эссе на несколько страничек мелкого почерка. С другой стороны, мы получили весьма зрелищное и душевное окончание многолетней саги, увидели любимых персонажей, вдоволь посмеялись и даже местами поплакали. А раз фильм был способен вызвать такую гамму эмоций, значит создатели сделали почти все правильно.\"\n",
        "    correct = \"Positive\"\n",
        "    model_out = model_runner.predict(review)\n",
        "    print(f'{\"V\" if class_names[model_out]==correct else \"X\"} Результат модели: This review about \"{header}\" is {class_names[model_out]} (in fact it is {correct})')\n",
        "\n",
        "# Отрицательный Justice League, 2017\n",
        "    header = \"Justice League, 2017\"\n",
        "    review = \"И тут все дело в подаче. Понятно что в комиксах вселенная DC давно существует и там Бэтмен и Лига Справедливости спина к спине оберегают Землю. Но в кино мире - Бэтмен всегда был одиночкой, и соперники у него были под стать - без исключительных суперспособностей. И теперь DC пытается впихнуть героя, который в массовом сознании прослыл 'одиноким рейнджером' к героям у которых настоящие суперсилы и суперспособности. Бэтмен на их фоне выглядит ну совсем никак. Но даже не смотря на диссонанс с Бэтменом у DC большие проблемы: 'Лига справедливости', по сравнению с 'Мстителями' выглядит просто дешевой поделкой, вроде все как у них но нет того ощущения постоянного драйва, герои раскрыты однобоко, а вечный пафос раздражает - так как ощущается чем то инородным и неуместным к данной картине. Сюжет в Лиге очень плоский, спецэффекты на уровне, но если их не сдабривать нужным эмоциональным фоном, они начинают смотреться как нарезка трюков. Конечно были в фильмы проблески чего то хорошего, но на общем фоне картины они не осели в памяти, зато осело разочарование от завышенных ожиданий\"\n",
        "    correct = \"Negative\"\n",
        "    model_out = model_runner.predict(review)\n",
        "    print(f'{\"V\" if class_names[model_out]==correct else \"X\"} Результат модели: This review about \"{header}\" is {class_names[model_out]} (in fact it is {correct})')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "======================= ruBert with ds_10% dataset volume =========================\nV Результат модели: This review about \"Morbius 2022\" is Negative (in fact it is Negative)\nV Результат модели: This review about \"Avengers: Endgame, 2019\" is Positive (in fact it is Positive)\nV Результат модели: This review about \"Justice League, 2017\" is Negative (in fact it is Negative)\n======================= ruBert with ds_20% dataset volume =========================\nX Результат модели: This review about \"Morbius 2022\" is Neutral (in fact it is Negative)\nV Результат модели: This review about \"Avengers: Endgame, 2019\" is Positive (in fact it is Positive)\nV Результат модели: This review about \"Justice League, 2017\" is Negative (in fact it is Negative)\n======================= ruBert with ds_50% dataset volume =========================\nX Результат модели: This review about \"Morbius 2022\" is Neutral (in fact it is Negative)\nV Результат модели: This review about \"Avengers: Endgame, 2019\" is Positive (in fact it is Positive)\nX Результат модели: This review about \"Justice League, 2017\" is Neutral (in fact it is Negative)\n======================= ruBert with ds_100% dataset volume =========================\nX Результат модели: This review about \"Morbius 2022\" is Neutral (in fact it is Negative)\nV Результат модели: This review about \"Avengers: Endgame, 2019\" is Positive (in fact it is Positive)\nV Результат модели: This review about \"Justice League, 2017\" is Negative (in fact it is Negative)\n"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jukit_cell_id": "7VYXas56u0",
        "id": "AXLp-WF2JEmQ"
      },
      "source": [
        "Результат модели LSTM, созданной ранее в [ДЗ](https://colab.research.google.com/drive/1uSbb1xCYA3M2144oq0rBUdJN_NcTskED?usp=sharing)  показал точность в 73% на тестовой выборке.  \n",
        "В итоге получили точность модели BERT с предобучением и токенизатором от ruBERT   в 86% при обучении на 100% датасете. Обучение на 10% размере датасета показывает такую же точность модели как и LTSM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jukit_cell_id": "tXIP4gy2Ev",
        "id": "ibi8wmo1JEmQ"
      },
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "python",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}